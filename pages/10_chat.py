import streamlit as st
import google.generativeai as genai
from dotenv import load_dotenv
import os
import pypdf
import pandas as pd
from io import BytesIO
from PIL import Image
import base64
import asyncio
from time import sleep
from concurrent.futures import ThreadPoolExecutor
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)

# Configura√ß√µes e Constantes
MAX_CHUNK_SIZE = 30000
MAX_CONCURRENT_REQUESTS = 3
REQUEST_INTERVAL = 1.0

# Configura√ß√£o da p√°gina Streamlit
st.set_page_config(
    page_title="Assistente Inteligente de Documentos",
    page_icon="ü§ñ",
    layout="wide"
)

# Carrega vari√°veis de ambiente
load_dotenv()

class RateLimitError(Exception):
    """Exce√ß√£o customizada para rate limiting."""
    def __init__(self, retry_after=None):
        self.retry_after = retry_after
        super().__init__(f"Rate limit exceeded. Retry after {retry_after} seconds")

class RequestManager:
    """Gerencia requisi√ß√µes com rate limiting."""
    def __init__(self):
        self.last_request_time = 0
        self.semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
        self.lock = asyncio.Lock()
    
    async def wait_for_rate_limit(self):
        """Espera o tempo necess√°rio entre requisi√ß√µes."""
        async with self.lock:
            current_time = asyncio.get_event_loop().time()
            time_since_last = current_time - self.last_request_time
            if time_since_last < REQUEST_INTERVAL:
                await asyncio.sleep(REQUEST_INTERVAL - time_since_last)
            self.last_request_time = asyncio.get_event_loop().time()

def configure_gemini():
    """Configura a API do Gemini."""
    try:
        api_key = st.secrets.get("GOOGLE_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if not api_key:
            st.warning("API key n√£o encontrada. Configure sua API key no sidebar.")
            return False
        genai.configure(api_key=api_key)
        return True
    except Exception as e:
        st.error(f"Erro ao configurar a API Gemini: {e}")
        return False

def chunk_text(text, max_size=MAX_CHUNK_SIZE):
    """Divide texto em chunks menores de forma inteligente."""
    if not text or len(text) <= max_size:
        return [text] if text else []
    
    chunks = []
    current_chunk = ""
    
    paragraphs = text.split('\n\n')
    
    for paragraph in paragraphs:
        if len(current_chunk) + len(paragraph) <= max_size:
            current_chunk += paragraph + '\n\n'
        else:
            if len(paragraph) > max_size:
                sentences = paragraph.replace('\n', ' ').split('. ')
                for sentence in sentences:
                    if len(current_chunk) + len(sentence) > max_size:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                            current_chunk = ""
                    current_chunk += sentence + '. '
            else:
                chunks.append(current_chunk.strip())
                current_chunk = paragraph + '\n\n'
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks

def extract_text_from_pdf(pdf_file):
    """Extrai texto de um arquivo PDF."""
    try:
        if not pdf_file:
            raise ValueError("Nenhum arquivo PDF fornecido")
        
        pdf_reader = pypdf.PdfReader(pdf_file)
        text = []
        total_pages = len(pdf_reader.pages)
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for page_num, page in enumerate(pdf_reader.pages, 1):
            try:
                status_text.text(f"Processando p√°gina {page_num} de {total_pages}")
                page_text = page.extract_text()
                if page_text:
                    text.append(f"\n=== P√°gina {page_num} ===\n{page_text}")
                progress_bar.progress(page_num / total_pages)
            except Exception as e:
                st.warning(f"Erro ao extrair texto da p√°gina {page_num}: {e}")
                continue
                
        progress_bar.empty()
        status_text.empty()
        
        return "\n".join(text) if text else None
    except Exception as e:
        st.error(f"Erro ao processar PDF: {e}")
        return None

def extract_text_from_excel(excel_file):
    """Extrai texto de um arquivo Excel."""
    try:
        if not excel_file:
            raise ValueError("Nenhum arquivo Excel fornecido")
        
        excel_data = pd.ExcelFile(excel_file)
        all_sheets_text = []
        total_sheets = len(excel_data.sheet_names)
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for idx, sheet_name in enumerate(excel_data.sheet_names, 1):
            try:
                status_text.text(f"Processando planilha {idx} de {total_sheets}: {sheet_name}")
                df = pd.read_excel(excel_file, sheet_name=sheet_name)
                if not df.empty:
                    all_sheets_text.append(f"\n=== Planilha: {sheet_name} ===\n{df.to_string()}")
                progress_bar.progress(idx / total_sheets)
            except Exception as e:
                st.warning(f"Erro ao processar planilha '{sheet_name}': {e}")
                continue
                
        progress_bar.empty()
        status_text.empty()
        
        return "\n".join(all_sheets_text) if all_sheets_text else None
    except Exception as e:
        st.error(f"Erro ao processar Excel: {e}")
        return None

def process_image(image_file):
    """Processa e codifica a imagem para a API."""
    try:
        if not image_file:
            raise ValueError("Nenhuma imagem fornecida")
        
        image = Image.open(image_file)
        if image.mode not in ('RGB', 'L'):
            image = image.convert('RGB')
        
        max_size = 1600
        if max(image.size) > max_size:
            ratio = max_size / max(image.size)
            new_size = tuple(int(dim * ratio) for dim in image.size)
            image = image.resize(new_size, Image.Resampling.LANCZOS)
        
        buffered = BytesIO()
        image.save(buffered, format="JPEG", quality=85)
        return base64.b64encode(buffered.getvalue()).decode('utf-8')
    except Exception as e:
        st.error(f"Erro ao processar imagem: {e}")
        return None

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type(RateLimitError)  # Aqui estava o erro: use "retry=" ao inv√©s de apenas passando o argumento direto
)

async def process_chunk(chunk, question, model, request_manager, safety_settings):
    """Processa um chunk individual com rate limiting."""
    async with request_manager.semaphore:
        await request_manager.wait_for_rate_limit()
        try:
            prompt = f"""Analise este trecho do documento e responda √† pergunta considerando o contexto geral:

            Texto: {chunk}

            Pergunta: {question}

            Instru√ß√µes:
            1. Foque nas informa√ß√µes relevantes deste trecho
            2. Mantenha consist√™ncia com o contexto geral
            3. Indique se a informa√ß√£o parece incompleta ou precisa de mais contexto"""

            response = await asyncio.to_thread(
                model.generate_content,
                prompt,
                safety_settings=safety_settings
            )
            
            if response.text.strip() in ["True", "False"]:
                raise ValueError("Resposta inv√°lida recebida da API")
            
            return response.text
        except Exception as e:
            if "429" in str(e):
                raise RateLimitError(retry_after=60)
            raise

async def process_large_document(content, question, model, safety_settings):
    """Processa documento grande em chunks paralelos."""
    chunks = chunk_text(content)
    request_manager = RequestManager()
    
    total_chunks = len(chunks)
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    processed_chunks = 0
    tasks = []
    
    # Cria√ß√£o das tasks com todos os argumentos nomeados
    for chunk in chunks:
        task = process_chunk(
            chunk=chunk,
            question=question,
            model=model,
            request_manager=request_manager,
            safety_settings=safety_settings
        )
        tasks.append(task)
    
    responses = []
    async for task in asyncio.as_completed(tasks):
        try:
            response = await task
            responses.append(response)
            processed_chunks += 1
            progress_bar.progress(processed_chunks / total_chunks)
            status_text.text(f"Processando chunk {processed_chunks} de {total_chunks}")
        except Exception as e:
            st.warning(f"Erro ao processar chunk: {e}")
    
    progress_bar.empty()
    status_text.empty()
    
    if not responses:
        raise ValueError("N√£o foi poss√≠vel obter respostas v√°lidas")
    
    combined_response = "\n\n".join(responses)
    
    summary_prompt = f"""Sumarize as seguintes respostas em uma √∫nica resposta coerente:

    Respostas: {combined_response}

    Pergunta original: {question}

    Instru√ß√µes:
    1. Combine informa√ß√µes complementares
    2. Resolva poss√≠veis contradi√ß√µes
    3. Mantenha apenas informa√ß√µes relevantes
    4. Organize em uma resposta clara e concisa"""
    
    # Uso de argumentos nomeados na chamada de generate_content
    final_response = await asyncio.to_thread(
        model.generate_content,
        contents=summary_prompt,
        safety_settings=safety_settings
    )
    
    return final_response.text

def get_gemini_response(content, question, file_type=None, context=None):
    """Obt√©m resposta do Gemini com integra√ß√£o de conhecimento base."""
    try:
        if not content or not question:
            raise ValueError("Conte√∫do ou pergunta n√£o fornecidos")

        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"}
        ]

        if file_type == "image":
            model = genai.GenerativeModel('gemini-1.5-flash')
            prompt = f"""Por favor, analise esta imagem e responda √† seguinte pergunta:

            Pergunta: {question}

            Instru√ß√µes espec√≠ficas:
            1. Use seu conhecimento geral para fornecer contexto adicional relevante
            2. Fa√ßa conex√µes com conceitos relacionados mesmo que n√£o estejam expl√≠citos na imagem
            3. Se identificar elementos na imagem, explique sua relev√¢ncia
            4. Forne√ßa explica√ß√µes detalhadas baseadas tanto na imagem quanto em seu conhecimento geral
            
            Contexto adicional: {context if context else 'Nenhum contexto adicional fornecido'}"""

            response = model.generate_content(
                contents=[prompt, {"mime_type": "image/jpeg", "data": content}],
                safety_settings=safety_settings
            )
            return response.text
        else:
            model = genai.GenerativeModel('gemini-pro')
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                response = loop.run_until_complete(
                    process_large_document(content, question, model, safety_settings)
                )
                return response
            finally:
                loop.close()

    except Exception as e:
        error_message = str(e)
        if isinstance(e, RateLimitError):
            st.warning(f"Limite de requisi√ß√µes atingido. Aguardando {e.retry_after} segundos...")
            sleep(e.retry_after)
            return get_gemini_response(content, question, file_type, context)
        elif "403" in error_message:
            st.error("Erro de autentica√ß√£o. Verifique sua API key.")
        else:
            st.error(f"Erro ao obter resposta: {e}")
        return "N√£o foi poss√≠vel gerar uma resposta adequada. Por favor, tente novamente."

def init_session_state():
    """Inicializa vari√°veis da sess√£o."""
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "context" not in st.session_state:
        st.session_state.context = ""

def show_chat_history():
    """Exibe o hist√≥rico do chat."""
    if not st.session_state.chat_history:
        st.info("Fa√ßa uma pergunta para come√ßar a conversa!")
        return

    for question, answer in st.session_state.chat_history:
        with st.container():
            col1, col2 = st.columns([1, 2])
            with col1:
                st.markdown(
                    f"""<div style='background-color:#f0f2f6; 
                    padding: 10px; border-radius: 10px; 
                    margin-bottom: 10px;'>
                    <strong>Pergunta:</strong><br>{question}</div>""",
                    unsafe_allow_html=True
                )
            with col2:
                st.markdown(
                    f"""<div style='background-color:#d4edda; 
                    padding: 10px; border-radius: 10px; 
                    margin-bottom: 10px;'>
                    <strong>Resposta:</strong><br>{answer}</div>""",
                    unsafe_allow_html=True
                )

def main():
    st.title("ü§ñ Assistente Inteligente de Documentos")
    st.markdown("""
    Este assistente usa IA avan√ßada para analisar seus documentos e responder perguntas,
    combinando o conte√∫do do documento com conhecimento geral para respostas mais completas.
    """)

    with st.sidebar:
        st.header("‚öôÔ∏è Configura√ß√£o")
        api_key = st.text_input(
            "API Key do Google",
            type="password",
            help="Insira sua API key se n√£o estiver configurada no .env"
        )
        if api_key:
            genai.configure(api_key=api_key)

        st.header("üéØ Contexto Adicional")
        st.session_state.context = st.text_area(
            "Adicione contexto para melhorar as respostas",
            help="Informa√ß√µes adicionais que podem ajudar a IA a entender melhor suas perguntas"
        )

        st.header("‚ö° Configura√ß√µes Avan√ßadas")
        max_chunk_size = st.slider(
            "Tamanho m√°ximo do chunk (caracteres)",
            min_value=1000,
            max_value=50000,
            value=MAX_CHUNK_SIZE,
            step=1000,
            help="Ajuste o tamanho dos chunks para processamento. Valores menores podem ser mais precisos mas mais lentos."
        )

        max_concurrent = st.slider(
            "M√°ximo de requisi√ß√µes simult√¢neas",
            min_value=1,
            max_value=5,
            value=MAX_CONCURRENT_REQUESTS,
            help="Ajuste o n√∫mero m√°ximo de requisi√ß√µes simult√¢neas. Valores maiores podem ser mais r√°pidos mas podem atingir limites da API."
        )

    if not configure_gemini():
        st.stop()

    init_session_state()

    uploaded_file = st.file_uploader(
        "üì§ Carregue seu arquivo",
        type=["pdf", "xlsx", "xls", "png", "jpg", "jpeg", "bmp", "tiff"],
        help="Arquivos suportados: PDF, Excel, PNG, JPG, JPEG, BMP, TIFF"
    )

    if uploaded_file:
        file_type = None
        file_extension = os.path.splitext(uploaded_file.name)[1].lower()

        if file_extension == ".pdf":
            file_type = "pdf"
            with st.spinner("üìÑ Processando PDF..."):
                text = extract_text_from_pdf(uploaded_file)
                if text:
                    content = text
                    st.success("‚úÖ PDF processado com sucesso!")
                    with st.expander("üëÄ Visualizar conte√∫do do PDF"):
                        st.text_area("Conte√∫do do PDF", value=content, height=200, label_visibility="collapsed")

        elif file_extension in [".xlsx", ".xls"]:
            file_type = "excel"
            with st.spinner("üìä Processando Excel..."):
                text = extract_text_from_excel(uploaded_file)
                if text:
                    content = text
                    st.success("‚úÖ Excel processado com sucesso!")
                    with st.expander("üëÄ Visualizar conte√∫do do Excel"):
                        st.text_area("Conte√∫do do Excel", value=content, height=200, label_visibility="collapsed")

        elif file_extension in [".png", ".jpg", ".jpeg", ".bmp", ".tiff"]:
            file_type = "image"
            with st.spinner("üñºÔ∏è Processando imagem..."):
                encoded_image = process_image(uploaded_file)
                if encoded_image:
                    content = encoded_image
                    st.success("‚úÖ Imagem processada com sucesso!")
                    st.image(uploaded_file, caption="Imagem carregada", use_container_width=True)

        if 'content' in locals():  # Verifica se content foi definido
            st.markdown("### üí≠ Fa√ßa sua pergunta")
            
            # Adiciona sugest√µes de perguntas baseadas no tipo de arquivo
            suggestions = {
                "pdf": [
                    "Qual √© o tema principal deste documento?",
                    "Pode fazer um resumo dos pontos principais?",
                    "Quais s√£o as conclus√µes mais importantes?"
                ],
                "excel": [
                    "Quais s√£o os padr√µes observados nos dados?",
                    "Pode identificar tend√™ncias importantes?",
                    "Quais s√£o os valores mais significativos?"
                ],
                "image": [
                    "O que voc√™ pode me dizer sobre esta imagem?",
                    "Quais s√£o os elementos principais desta imagem?",
                    "Que tipo de ambiente/contexto esta imagem mostra?"
                ]
            }

            if file_type in suggestions:
                with st.expander("üí° Sugest√µes de perguntas"):
                    for suggestion in suggestions[file_type]:
                        if st.button(suggestion, key=suggestion):
                            st.session_state.question = suggestion

            question = st.text_input(
                "Digite sua pergunta sobre o documento",
                key="question",
                help="Seja espec√≠fico em sua pergunta. O assistente combinar√° o conte√∫do do documento com conhecimento geral."
            )

            col1, col2, col3 = st.columns([1, 1, 4])
            with col1:
                ask_button = st.button(
                    "üöÄ Perguntar",
                    use_container_width=True,
                    help="Clique para enviar sua pergunta"
                )

            with col2:
                clear_button = st.button(
                    "üóëÔ∏è Limpar Hist√≥rico",
                    use_container_width=True,
                    help="Limpa todo o hist√≥rico de perguntas e respostas"
                )

            if clear_button:
                st.session_state.chat_history = []
                st.rerun()

            if ask_button or question:
                if question.strip():
                    with st.spinner("ü§î Analisando e gerando resposta..."):
                        answer = get_gemini_response(
                            content, 
                            question, 
                            file_type, 
                            st.session_state.context
                        )
                        if answer:
                            st.session_state.chat_history.append((question, answer))
                else:
                    st.warning("‚ö†Ô∏è Por favor, digite uma pergunta.")

            if st.session_state.chat_history:
                st.markdown("---")
                st.markdown("### üìù Hist√≥rico da Conversa")
                show_chat_history()

            # Adiciona op√ß√£o de exportar hist√≥rico
            if st.session_state.chat_history:
                if st.button("üì• Exportar Hist√≥rico"):
                    export_text = "Hist√≥rico de Perguntas e Respostas\n\n"
                    for q, a in st.session_state.chat_history:
                        export_text += f"Pergunta: {q}\n\nResposta: {a}\n\n---\n\n"
                    
                    st.download_button(
                        label="üíæ Baixar Hist√≥rico",
                        data=export_text.encode('utf-8'),
                        file_name="historico_chat.txt",
                        mime="text/plain"
                    )

if __name__ == "__main__":
    main()